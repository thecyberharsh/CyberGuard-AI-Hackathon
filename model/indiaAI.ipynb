{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Cyber Bullying  Stalking  Sexting</td>\n",
       "      <td>I had continue received random calls and abusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>The above fraudster is continuously messaging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>He is acting like a police and demanding for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Job Fraud</td>\n",
       "      <td>In apna Job I have applied for job interview f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>I received a call from lady stating that she w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                category                       sub_category  \\\n",
       "0  Online and Social Media Related Crime  Cyber Bullying  Stalking  Sexting   \n",
       "1                 Online Financial Fraud                  Fraud CallVishing   \n",
       "2               Online Gambling  Betting           Online Gambling  Betting   \n",
       "3  Online and Social Media Related Crime                   Online Job Fraud   \n",
       "4                 Online Financial Fraud                  Fraud CallVishing   \n",
       "\n",
       "                                  crimeaditionalinfo  \n",
       "0  I had continue received random calls and abusi...  \n",
       "1  The above fraudster is continuously messaging ...  \n",
       "2  He is acting like a police and demanding for m...  \n",
       "3  In apna Job I have applied for job interview f...  \n",
       "4  I received a call from lady stating that she w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['crimeaditionalinfo'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category              0\n",
       "sub_category          0\n",
       "crimeaditionalinfo    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # Check if the text is NaN\n",
    "        return []  # Return an empty list for missing values\n",
    "    text = str(text).lower()  # Convert to string and lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = text.split()  # Tokenize by whitespace\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data['tokens'] = train_data['crimeaditionalinfo'].apply(preprocess_text)\n",
    "test_data['tokens'] = test_data['crimeaditionalinfo'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "sentences = train_data['tokens'].tolist()\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,   # Embedding dimension\n",
    "    window=5,          # Context window size\n",
    "    min_count=2,       # Ignore words with frequency lower than 2\n",
    "    workers=4,         # Number of CPU cores\n",
    "    sg=1               # Use skip-gram\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "word2vec_model.save(\"custom_word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)  # Averaging word embeddings\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # For empty or OOV-only sentences\n",
    "\n",
    "# Generate sentence embeddings for each sentence in the training data\n",
    "train_data['sentence_embedding'] = train_data['tokens'].apply(lambda x: get_sentence_embedding(x, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.88021535e-02,  2.49270484e-01,  6.92878217e-02,  2.11779643e-02,\n",
       "        6.41876012e-02, -1.03904888e-01,  9.61883068e-02,  4.18564916e-01,\n",
       "       -5.49675012e-03,  5.46150748e-03,  7.18926117e-02, -1.28895611e-01,\n",
       "        1.33500502e-01,  5.03949774e-03, -3.21658961e-02, -1.97155431e-01,\n",
       "        1.93899781e-01, -1.23419344e-01,  2.60990448e-02, -1.03132665e-01,\n",
       "        1.82077866e-02,  1.68189220e-02,  9.87077057e-02,  8.73014107e-02,\n",
       "        5.64403795e-02,  5.48228174e-02, -1.78125292e-01,  1.37511373e-01,\n",
       "       -6.45054132e-02, -1.21994115e-01, -2.04318389e-02, -1.41354678e-02,\n",
       "        2.88357865e-02,  4.69467118e-02, -6.93118349e-02,  1.41741782e-01,\n",
       "       -1.44890044e-02, -2.37292171e-01,  5.64138405e-02,  2.21521966e-02,\n",
       "       -7.07385615e-02,  2.75654551e-02, -9.08959135e-02, -1.41900793e-01,\n",
       "       -2.44009253e-02,  1.93675771e-01, -5.17208539e-02, -8.14843997e-02,\n",
       "       -5.58990538e-02,  8.26207474e-02,  1.52379677e-01,  2.46693548e-02,\n",
       "       -2.42827684e-02,  5.54288477e-02, -2.79398207e-02, -1.14131868e-02,\n",
       "        8.39299038e-02, -8.17627832e-03, -3.96294706e-02,  4.57845442e-02,\n",
       "       -1.06918648e-01, -1.59593582e-01,  1.14424359e-02,  2.23376565e-02,\n",
       "        6.43604621e-02,  4.83367126e-03, -8.82610306e-02,  1.75377745e-02,\n",
       "       -2.01250747e-01,  3.44336964e-02,  3.09932418e-02,  7.70311281e-02,\n",
       "        7.50693679e-02, -1.42984018e-01,  6.42891452e-02, -9.11854848e-04,\n",
       "       -8.12779739e-02, -1.54882083e-02, -1.04669802e-01,  3.42741385e-02,\n",
       "       -5.11695035e-02, -7.29623884e-02,  9.18057282e-03,  3.43198806e-01,\n",
       "       -8.82600471e-02,  6.30913302e-02, -1.84661895e-03, -1.11529432e-01,\n",
       "        1.48792118e-01,  1.37705579e-01,  1.83181033e-01, -7.70761445e-03,\n",
       "        1.61627680e-01, -6.24002330e-02,  1.98875144e-01,  9.04781520e-02,\n",
       "        1.93551090e-02, -1.59484532e-03, -4.38819081e-02, -1.14817563e-02,\n",
       "       -8.26281905e-02, -2.49876566e-02,  4.37327102e-02, -6.18249970e-03,\n",
       "        9.90913361e-02, -8.75314921e-02, -1.41693847e-02,  1.40832573e-01,\n",
       "       -9.79997814e-02,  4.96338606e-02, -2.51572251e-01, -5.62366880e-02,\n",
       "       -1.60860285e-01,  2.03046232e-01,  1.24704443e-01,  1.46017596e-01,\n",
       "        3.05203069e-02,  7.41143003e-02,  2.44950995e-01, -2.18684658e-01,\n",
       "       -8.85732919e-02,  1.38623193e-01,  5.51025346e-02, -1.43972680e-01,\n",
       "       -7.68826529e-02,  1.56239197e-01, -1.50717750e-01, -1.82973713e-01,\n",
       "       -5.37128039e-02,  1.04497805e-01,  9.61155295e-02,  1.20304450e-01,\n",
       "        1.50258213e-01, -2.04006255e-01,  1.64654747e-01,  1.12454750e-01,\n",
       "        1.63054006e-04,  2.11880561e-02, -9.53147411e-02, -1.71943858e-01,\n",
       "        7.33773038e-02, -1.62849814e-01, -5.80932871e-02,  7.88642392e-02,\n",
       "        3.05833034e-02, -1.31320104e-01, -1.88714609e-01, -7.64533952e-02,\n",
       "        2.87782103e-01, -8.72531384e-02,  9.95947514e-03, -1.97221860e-01,\n",
       "        5.47189191e-02,  8.34981427e-02, -5.77361044e-03,  1.30529970e-01,\n",
       "       -3.97094190e-02, -1.62762076e-01, -1.17542006e-01,  2.69881070e-01,\n",
       "       -2.85761300e-02,  2.05300346e-01, -2.03157842e-01,  1.24639191e-01,\n",
       "        7.01010004e-02,  5.43330386e-02, -7.91051611e-02, -6.48066327e-02,\n",
       "        1.44206330e-01,  2.33113706e-01, -6.20813714e-03,  2.73474641e-02,\n",
       "        7.58776739e-02,  2.09590331e-01,  5.71730025e-02,  4.43686955e-02,\n",
       "        1.78877085e-01, -1.60699412e-01,  1.07444979e-01,  7.67578334e-02,\n",
       "       -9.17360857e-02,  1.26990750e-01, -1.40717506e-01, -4.24611270e-02,\n",
       "       -1.15609698e-01,  8.71525034e-02,  1.92082182e-01,  3.12272936e-01,\n",
       "        9.39584523e-02, -2.49337554e-01,  1.82832088e-02, -5.55083081e-02,\n",
       "       -1.56799436e-01,  5.78627586e-02,  1.86131671e-02, -7.96030909e-02,\n",
       "       -8.86550471e-02, -2.10795254e-01, -1.53559238e-01,  8.95931292e-03,\n",
       "       -1.59736156e-01,  5.64029887e-02,  1.15739681e-01, -2.08482802e-01,\n",
       "        6.80421144e-02, -3.17102745e-02,  9.66088325e-02,  8.13420862e-02,\n",
       "       -2.39657741e-02, -7.05554103e-03, -1.23356923e-01, -1.73578203e-01,\n",
       "        5.35617098e-02, -2.46369839e-01,  1.21391878e-01, -8.67197812e-02,\n",
       "       -1.03156596e-01, -3.64750654e-01, -1.44463181e-01, -2.36634344e-01,\n",
       "        1.54300287e-01,  1.36835545e-01, -2.03574181e-01, -2.06334859e-01,\n",
       "       -4.00872640e-02, -1.67276636e-01,  1.13127179e-01, -5.01419455e-02,\n",
       "        1.89541299e-02,  7.56479278e-02,  1.06609151e-01, -1.20682269e-01,\n",
       "       -1.24440290e-01,  3.84833217e-02, -1.31757662e-01, -6.13642782e-02,\n",
       "       -4.57558334e-02,  2.16017999e-02,  1.20490134e-01, -3.03461969e-01,\n",
       "        1.32144094e-01, -6.18153699e-02,  8.97650644e-02,  5.35143726e-03,\n",
       "        7.07435682e-02, -5.74969575e-02,  3.97787578e-02, -3.14153619e-02,\n",
       "       -8.59421566e-02,  1.53901488e-01,  1.19215347e-01,  6.95342273e-02,\n",
       "        1.29981741e-01,  9.15728658e-02, -1.61957189e-01, -1.46618500e-01,\n",
       "        2.05776840e-01,  5.52791990e-02, -3.98557276e-01, -5.32487780e-02,\n",
       "        2.52055675e-02,  2.58129556e-02,  8.12843293e-02, -2.05186352e-01,\n",
       "       -2.18754858e-02,  4.46582511e-02,  7.77387843e-02,  7.30697885e-02,\n",
       "       -2.18778893e-01,  1.10897616e-01, -3.30565348e-02, -7.17379525e-02,\n",
       "        1.60027258e-02, -1.22724950e-01,  1.36108279e-01,  5.48547991e-02,\n",
       "        1.21945463e-01,  1.39126018e-01, -1.76418215e-01, -2.76181903e-02,\n",
       "        1.08893113e-02, -5.23362942e-02, -8.30552503e-02,  5.64277805e-02,\n",
       "        4.76343781e-02,  8.08168203e-02, -1.38441339e-01,  1.84939921e-01,\n",
       "        5.55886962e-02,  1.85689345e-01,  1.14004739e-01,  1.45931929e-01,\n",
       "        7.90025741e-02, -9.01382975e-03,  1.78072944e-01,  1.26673043e-01,\n",
       "        5.14298212e-03, -2.89071202e-02,  4.76428494e-02, -2.01715216e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentence_embedding'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ana\\envs\\sammy\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7825298022371776\n",
      "recall score  0.7825298022371776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare data for model training\n",
    "X_train = np.vstack(train_data['sentence_embedding'].values)\n",
    "y_train = train_data['category']  # Assuming category is the label column\n",
    "\n",
    "# Train a classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training data to evaluate\n",
    "y_pred = classifier.predict(X_train)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"recall score \",recall_score(y_train, y_pred,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['sentence_embedding'] = test_data['tokens'].apply(lambda x: get_sentence_embedding(x, word2vec_model))\n",
    "X_test = np.vstack(test_data['sentence_embedding'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7223093919113645\n",
      "recall score  0.7223093919113645\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_test = test_data['category']  # Replace with the actual label column in your test data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"recall score \",recall_score(y_test, y_test_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sammy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
